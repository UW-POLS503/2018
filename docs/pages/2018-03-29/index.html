<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="generator" content="Hugo 0.31.1" />

      
      <title>Bayes&#39; Theorem Examples - POLS 503: Advanced Quantitative Political Methodology</title>
      <meta property="og:title" content="Bayes&#39; Theorem Examples - POLS 503: Advanced Quantitative Political Methodology">
    

    
      
    

    

    

    


<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



    <link rel="stylesheet" href="/2018/bootstrap-4.0.0-beta.2-dist/css/bootstrap.min.css">

    <script defer src="/2018/fontawesome/js/fontawesome-all.js"></script>


    <link rel="stylesheet" href="/2018/css/pols501.css" media="all">

  </head>
  <body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
  <a class="navbar-brand" href="/2018/">POLS 503 </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="navbarsExampleDefault">
    <ul class="navbar-nav mr-auto">
    
      <li class="nav-item">
        <a class="nav-link" href="/2018/assignments/">Assignments</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link" href="/2018/pages/">Pages</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link" href="/2018/schedule/">Schedule</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-link" href="/2018/">Syllabus</a>
      </li>
    
    
    </ul>
  </div>
</nav>



    
<main class="container">
    <h1>Bayes&#39; Theorem Examples</h1>

    <p>This document contains a discussion and several examples of Bayes’ Theorem.</p>
<div id="prerequisites" class="section level1">
<h1>Prerequisites</h1>
<pre class="r"><code>library(&quot;tidyverse&quot;)</code></pre>
<pre><code>## ── Attaching packages ─────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1     ✔ purrr   0.2.4
## ✔ tibble  1.4.2     ✔ dplyr   0.7.4
## ✔ tidyr   0.8.0     ✔ stringr 1.3.0
## ✔ readr   1.1.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(&quot;babynames&quot;)</code></pre>
</div>
<div id="bayes-theorem" class="section level1">
<h1>Bayes’ Theorem</h1>
<p>For events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>,
<span class="math display">\[
\underbrace{\Pr(A | B)}_{\text{posterior}} = \frac{\overbrace{\Pr(B | A)}^{\text{likelihood}} \overbrace{\Pr(A)}^{\text{prior}}}{\underbrace{\Pr(B)}_{\text{marginal likelihood}}},
\]</span>
where <span class="math inline">\(\Pr(B) \neq 0\)</span>.</p>
<p>For discrete random variables <span class="math inline">\(X\)</span> which takes values in the set <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(Y\)</span> which takes values in the set <span class="math inline">\(\mathcal{Y}\)</span>,
Bayes’ Theorem can be written as,
<span class="math display">\[
p_{Y|X}(X = x|Y = y) = \frac{p_{Y|X}(Y = y|X = x) p_X(X = x)}{p_Y(Y = y)} = \frac{p_{Y|X}(Y = y|X = x) p_X(X = x)}{\sum_{x \in \mathcal{x}} p_{Y|X}(Y = y|X = x) p_X(X = x)} 
\]</span></p>
<p>For continuous random variables <span class="math inline">\(X\)</span> with support <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with support <span class="math inline">\(\mathcal{Y}\)</span>,
Bayes’ Theorem can be written as,
<span class="math display">\[
p_{Y|X}(x|Y = y) = \frac{p_{Y|X = x}(y) p_X(x)}{p_Y(y)} = \frac{p_{Y|X = x}(y) p_X(x)}{\int_{x \in \mathcal{x}} p_{Y|X = x}(y) p_X(x) dx} 
\]</span>
Though there are deeper differences between discrete and continuous probability theory, the primary difference in the equations for Bayes’ Theorem with discrete or continuous random variables is whether summation or integration is used to calculate the marginal likelihood.</p>
</div>
<div id="examples" class="section level1">
<h1>Examples</h1>
<div id="taxi-cab-problem" class="section level2">
<h2>Taxi-Cab Problem</h2>
<blockquote>
<p>Suppose you were told that a taxi-cab was involved in a hit-and-run accident one night.
Of the taxi-cabs in the city, 85% belonged to the Green company and 15% to the Blue company.
You are then asked to estimate the likelihood that the hit-and-run accident involved a green taxi-cab (all else being equal).<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
</blockquote>
<p>What is the probability that the taxi-cab involved in the hit and run is blue?
It is 85%, since we have no other information.</p>
<blockquote>
<p>You are then told that an eyewitness had identified the cab as a blue cab.
But when her ability to identify cabs under appropriate visibility conditions was tested, she was wrong 20% of the time.
What is the probability that the cab is blue?</p>
</blockquote>
<p>Let <span class="math inline">\(H\)</span> bet the event that</p>
<p>Let <span class="math inline">\(H_B\)</span> (<span class="math inline">\(H_G\)</span>) be the event that a blue (green) cab committed the hit and run.
Let <span class="math inline">\(W_B\)</span> (<span class="math inline">\(W_G\)</span>) be the event that the witness reported that a blue (green) cab committed the hit and run.</p>
<p>We are interested in <span class="math inline">\(\Pr(H_B | W_B)\)</span>, the probability that a blue cab committed the hit and run given that the witness reported a blue cab committing the hit and run.
<span class="math display">\[
\Pr(H_B | W_B) = \frac{\Pr(W_B | H_B) \Pr(H_B)}{\Pr(W_B)} = \frac{\Pr(W_B | H_B) \Pr(H_B)}{\Pr(W_B | H_B) \Pr(H_B) + \Pr(W_B | H_G) \Pr(H_G)}.
\]</span></p>
<p>The prior probabilities of the color of the cab come are the proportions of cabs in the city,
<span class="math display">\[
\begin{aligned}
\Pr(H_B) &amp;= 0.15 ,\\
\Pr(H_G) &amp;= 0.85 .
\end{aligned}
\]</span></p>
<p>The conditional probabilities are,
<span class="math display">\[
\begin{aligned}[t]
p(W_B | H_B) &amp;= 0.8 , \\
p(W_B | H_G) &amp;= 0.2 .
\end{aligned}
\]</span></p>
<p>The marginal likelihood (model evidence) is the overall probability that a cab is reported to be blue.
This considers both the probabilities that a witness reports that the cab is blue when it is blue and reports that it is blue when it is green.
<span class="math display">\[
\begin{aligned}[t]
\Pr(W_B) = \Pr(W_B | H_B) \Pr(H_B) + \Pr(W_B | H_B) \Pr(H_B)
\end{aligned} 
\]</span></p>
<p>To calculate the posterior distribution, put the prior and likelihoods into a table.</p>
<pre class="r"><code>cabs &lt;- tribble(
  ~ color, ~ prior, ~ likelihood,
  &quot;blue&quot;,      0.15,        0.8, 
  &quot;green&quot;,     0.85,        0.2
)</code></pre>
<p>Calculate the marginal probability.</p>
<pre class="r"><code>cabs %&gt;%
  mutate(
    marginal = sum(likelihood * prior),
    posterior = likelihood * prior / marginal
  )</code></pre>
<pre><code>## # A tibble: 2 x 5
##   color prior likelihood marginal posterior
##   &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 blue  0.150      0.800    0.290     0.414
## 2 green 0.850      0.200    0.290     0.586</code></pre>
<p><strong>Q:</strong> Suppose that you know that all cabs in the city are blue or green, but you don’t know the proportions of them.
You use the <a href="https://en.wikipedia.org/wiki/Principle_of_indifference">principle of indifference</a> to assign prior probabilities of,
<span class="math display">\[
\begin{aligned}[t]
p(H_B) = p(H_G) = 0.5 .
\end{aligned}
\]</span>
Suppose the witness report is the same, what is the probability that the cab committing the hit and run was blue.</p>
<p><strong>Q:</strong> A common answer to this question is “blue”. This mistake is often due to
ignoring the prior probability of an event, and interpreting <span class="math inline">\(P(H_B | W_B) = P(W_B | H_B)\)</span>. This is called the <a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">base-rate fallacy</a>?
What prior does the base-rate fallacy correspond to?
In other words, what prior is needed such that <span class="math inline">\(\Pr(H_B | W_B) = \Pr(W_B | H_B)\)</span>.</p>
<p><strong>Q:</strong> Suppose that there was was perfectly reliable video evidence of the hit and run, such that <span class="math inline">\(\Pr(W_B | H_B) = 1\)</span> and <span class="math inline">\(\Pr(W_B | H_G) = 0\)</span>.
What is the probability that the cab committing the hit and run was blue?</p>
<p><strong>Q:</strong> Suppose that the witness reports that the cab was “yellow”.
You know that there are no yellow cabs in the city, thus <span class="math inline">\(\Pr(H_Y) = 0\)</span>.
What is the probability that the cab committing the hit and run was yellow, given that the witness reports it being yellow?<br />
What level of accuracy would you require from the witness such that you believed that the cab committing the hit and run was yellow.</p>
<p><strong>Q:</strong> What level of accuracy would be required from the witness such that it is more probable that a green cab committed the hit and run than a blue cab?</p>
<p><strong>Q:</strong> There have been various proposals to quantify what is meant by “<a href="https://doi.org/10.1093/lpr/mgl015">beyond a reasonable doubt</a>”. But for the purpose of this question, let’s suppose that beyond a reasonable doubt is a probability greater or equal to 0.8. What level of accuracy is required from the witness to meet the reasonable doubt standard?</p>
</div>
</div>
<div id="non-citizen-voters" class="section level1">
<h1>Non-citizen voters</h1>
<p>Suppose a survey includes 20,000 respondents.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>
Of them 19,500 are citizens and 500 are not.
Suppose that 99.9% of the time, the survey question response is correct (citizens respond that they are citizens, and non-citizens respond that they are non-citizens).
The survey against voting records, which provides the estimate <span class="math inline">\(P(v = 1 | c = 0) = 0.7\)</span></p>
<p>What is the probability of being a non-citizen given that a person reported being a non-citizen?</p>
<pre class="r"><code>sample_size &lt;- 20000
non_citizens &lt;- 500
p_non_citizen &lt;- non_citizens / 20000
accuracy &lt;- 0.999
prior_citizen &lt;- 0.5

tribble(
  ~ citizen_reported, ~ citizen,           ~ prior,  ~ likelihood, 
                TRUE,      TRUE,     prior_citizen,      accuracy,
                TRUE,     FALSE,     prior_citizen,  1 - accuracy,
                FALSE,     TRUE, 1 - prior_citizen,      accuracy,
                FALSE,    FALSE, 1 - prior_citizen,  1 - accuracy
)</code></pre>
<pre><code>## # A tibble: 4 x 4
##   citizen_reported citizen prior likelihood
##   &lt;lgl&gt;            &lt;lgl&gt;   &lt;dbl&gt;      &lt;dbl&gt;
## 1 TRUE             TRUE    0.500    0.999  
## 2 TRUE             FALSE   0.500    0.00100
## 3 FALSE            TRUE    0.500    0.999  
## 4 FALSE            FALSE   0.500    0.00100</code></pre>
<ul>
<li>Given a respondent responded that they were a non-citizen, what is the probability that they are actually a non-citizen?</li>
<li>How many citizens do you expect to respond that they are non-citizens?</li>
<li>How many non-citizens do you expect to respond that they are citizens?</li>
<li>Is the prior reasonable? How would you choose a better prior? How much would
it affect the results?</li>
</ul>
<p>Suppose that citizens vote with 70% probability, and non-citizens never vote.</p>
<ul>
<li>With these assumptions, what is the probability that they are a non-citizen given that they voted?</li>
<li>What is the probability that someone voted given that they reported being a non-citizen in the survey?</li>
<li>What is the implication for studying rare events, such as non-citizen voting using surveys (not designed for that)?</li>
</ul>
</div>
<div id="why-most-research-findings-are-false" class="section level1">
<h1>Why most research findings are false</h1>
<p>Consider this simplified mode of scientific research.
Let <span class="math inline">\(H\)</span> (<span class="math inline">\(\lnot H\)</span>) be the event that a hypothesis is true (false).
Let <span class="math inline">\(D\)</span> (<span class="math inline">\(\lnot D\)</span>) be the result of a hypothesis test of <span class="math inline">\(H\)</span>.</p>
<p>Suppose that the test uses statistical significance level of <span class="math inline">\(\alpha = 0.05\)</span>
Since statistical significance controls the presence of type I error,
<span class="math display">\[
P(H | \lnot D) = \alpha = 0.05
\]</span></p>
<pre class="r"><code>alpha &lt;- 0.05</code></pre>
<p>Suppose that the test uses a power level of <span class="math inline">\(\beta = 0.8\)</span>.
Since power is <span class="math inline">\(1 - \Pr(\text{Type II error})\)</span>,
<span class="math display">\[
\Pr(H | D) = \beta = 0.8
\]</span></p>
<pre class="r"><code>beta &lt;- 0.8</code></pre>
<p>Given that information, suppose that you observe <span class="math inline">\(D\)</span>. Can you calculate <span class="math inline">\(\Pr(H | D)\)</span>?</p>
<p>No. By Bayes’ Theorem,
<span class="math display">\[
\Pr(H | D) = \frac{\Pr(D | H) \Pr(H)}{\Pr(D)}
\]</span>
We cannot calculate this because we do not know <span class="math inline">\(\Pr(H)\)</span>.</p>
<p>Suppose that a priori, many hypotheses are false, and use the following value from the Ionnides paper,
<span class="math display">\[
\Pr(H) = \gamma = 0.1 .
\]</span></p>
<p>With this information we can calculate
<span class="math display">\[
\Pr(H | D) = \frac{\Pr(D | H) \Pr(H)}{\Pr(D | H) \Pr(H) + \Pr(D | \lnot H) \Pr(\lnot H)}
\]</span></p>
<pre class="r"><code>p_theta &lt;- 0.1
science &lt;- tribble(
  ~ theta,     ~ x,       ~ prior, ~ likelihood,
     TRUE,    TRUE,       p_theta,         beta,
     TRUE,   FALSE,       p_theta,     1 - beta,      
    FALSE,    TRUE,   1 - p_theta,        alpha,
    FALSE,   FALSE,   1 - p_theta,    1 - alpha
)</code></pre>
<p>Calculate the posterior probability for each value of <code>theta</code>,
for the different cases of <code>x</code>:</p>
<pre class="r"><code>group_by(science, x) %&gt;%
  mutate(marginal   = sum(likelihood * prior),
         posterior = likelihood * prior / marginal
       ) %&gt;%
  arrange(x)</code></pre>
<pre><code>## # A tibble: 4 x 6
## # Groups:   x [2]
##   theta x     prior likelihood marginal posterior
##   &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 TRUE  FALSE 0.100     0.200     0.875    0.0229
## 2 FALSE FALSE 0.900     0.950     0.875    0.977 
## 3 TRUE  TRUE  0.100     0.800     0.125    0.640 
## 4 FALSE TRUE  0.900     0.0500    0.125    0.360</code></pre>
<p><strong>Questions:</strong></p>
<ul>
<li>p-value hacking is a process by which a research ensures that their test has a statistically significant result? What term does this affect? If you know a study was p-value hacked, what is the posterior distribution</li>
<li>Suppose a paper finds support for a novel and counter-intuitive theory. What parameter would that affect? Would it result in a higher or lower posterior probability?</li>
<li>Suppose a paper conducts a test of a well-established theory. What parameter would that affect? Would it result in a higher or lower posterior probability?</li>
<li>There <a href="https://osf.io/preprints/psyarxiv/mky9j">some suggestions</a> to reduce the p-value threshold to <span class="math inline">\(\alpha = 0.005\)</span>. What is the posterior probability of <span class="math inline">\(\Pr(H | D)\)</span> in that case?</li>
<li>Given the other parameters, what value of <span class="math inline">\(\alpha\)</span> would you need so that <span class="math inline">\(\Pr(H | D) \geq 0.95\)</span> ?</li>
<li>Many studies are under-powered. For example, <a href="https://www.nature.com/articles/nrn3475">this paper</a> finds that empirically, many neuroscience experiments have powers of 8% to 31%. Suppose that the experiment has a power of 20%. What is the posterior probability <span class="math inline">\(\Pr(H | D)\)</span>?</li>
<li>Given the other parameters, what value of <span class="math inline">\(\beta\)</span> would you need so that <span class="math inline">\(\Pr(H | D) \geq 0.95\)</span> ?</li>
<li>Given the original parameters, how many times would you have to replicate a study to get <span class="math inline">\(P(H | D_1, \dots, D_k) \geq 0.95\)</span>?</li>
<li>Suppose you run a study twice. Does <span class="math inline">\(P(H | D_1, \lnot D_2) = P(H | D_1, \lnot D_2)\)</span>? In other words, does the order in which evidence is received matter?<br />
</li>
<li>A study produces a statistically significant result, with a <span class="math inline">\(p\)</span>-value of 0.01. The PI explains the results to the press saying that there is only a 1% chance that the findings are false. Is that interpretation of the p-value correct? If not, why not?</li>
<li><strong>Advanced</strong> Calculate the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler</a> divergence between the prior and the posterior. Which event has more information, <span class="math inline">\(D\)</span> or <span class="math inline">\(\lnot D\)</span>?</li>
</ul>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><a href="https://www.economist.com/news/leaders/21588069-scientific-research-has-changed-world-now-it-needs-change-itself-how-science-goes-wrong" class="uri">https://www.economist.com/news/leaders/21588069-scientific-research-has-changed-world-now-it-needs-change-itself-how-science-goes-wrong</a></li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Example from Tversky, D. Kahneman, Evidential impact of base rates, in <em>Judgement under uncertainty: Heuristics and biases</em>, D. Kahneman, P. Slovic, A. Tversky (editors), Cambridge University Press, 1982.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>This example is from Stephen Ansolabehere,
Samantha Luks, Brian F. Schaffner, <a href="https://cces.gov.harvard.edu/news/perils-cherry-picking-low-frequency-events-large-sample-surveys">The Perils of Cherry Picking Low Frequency Events in Large Sample Surveys</a>.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>


</main>

    <footer class="footer bg-light text-muted">
      <div class="container">
        <p>Made with <a href="https://bookdown.org/yihui/blogdown/">Blogdown</a>,
           <a href="https://gohugo.io/">Hugo</a>,
            <a href="https://getbootstrap.com/">Bootstrap</a>, and
            <a href="https://fontawesome.com/">Fontawesome</a>.
            Source code on <a href="https://github.com/UW-POLS501/2018/"><i class="fab fa-github"></i></a>.</p>
        <p>
          This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>
        </p>
      </div>
    </footer>

    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/2018/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script src="/2018/js/jquery-3.2.1.slim.min.js"></script>
<script src="/2018/js/popper.js"></script>


<script src="/2018/bootstrap-4.0.0-beta.2-dist/js/bootstrap.min.js"></script>


  </body>
</html>
