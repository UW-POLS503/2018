# Ordinary Linear Regression

Ordinary least squares (OLS) finds the line (in 2-dimensions) or plane (> 2 dimensions) defined by $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_K$  that is "closest" to a set of $n$ points, $(y_1, x_{1,1}, \dots, x_{0, 1}),  \dots, (y_1, x_{1,1}, \dots, x_{0, 1})$, as measured by the "squared distance" (least squares) between the values of the response variable $y_i$ and the predicted value,
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_1 \dots + \hat{\beta}_K x_K .
$$
or alternatively, minimizing the squared errors, $\hat{\epsilon}_i$,
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_1 \dots + \hat{\beta}_K x_K .
$$

$$
\begin{aligned}[t]
\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_K  &=\argmin_{\tilde{\beta}_0, \tilde{\beta}_1, \dots \tilde{\beta}_{K}} \sum_{i = 1}^n \left(y_i - (\tilde{\beta}_0 + \tilde{\beta}_1 x_{i,1} + \cdots + \tilde{\beta}_K x_{i,K}) \right)
\end{aligned}
$$

How do we solve this? 

There are a variety of optimization methods to find the minimum (maximum) of functions.

One nice feature of linear regression is that the solution to this problems has a "closed form", meaning the solution can be calculated analytically so that there is an equation that provides the solution.
This contrasts with optimization problems where the optimum needs to be found numerically, by trying iteratively trying different values until a miminum is found.

In the bivariate case, the solution is,


In the multivariate case, it is 
$$
\hat{\beta} = \cov(X, Y) / \var(X, X) = \left( \mat{X} \mat{X}\T \right) \mat{X}\T \mat{Y}
$$

- $\left( \mat{X} \mat{X}\T \right) \approx \var(X, X)$
- $\left( \mat{X} \mat{Y}\T \right) \approx \cov(X, X)$

- Q: What happens to $N$? What if $\bar{X}$ is not 0? 
- A: N's cancel out. The constant de-means everything

What is needed? 

- 

# What does it all mean? 

Linear regression produces a conditional mean/expectation.

- This follows from minimizing squared errors
- E.g. What is a linear regression without any covariates? It is the mean of $y$.

We can think of it as a special case of a conditional expectation function (CEF).


# "Eyeballing Linear Regression"


# Matrix Algebra

## Variances

$$
Variance(X) = \E[(X_i - \bar{X})(X_i - \bar{X})] = E(X_i^2) - \bar{X}^2
$$

- If $\bar{X} = 0$, then $Var(X) - E(X_i^2)$
- If $\bar{X} = 0$ or $\bar{Y} = 0$, $cov(X, Y) = E(X, Y)$

Note that $E(X_i^2)$ is the average of $X_i^2$, which is
$$
\frac{\sum_i X_i X_i}{N} = \frac{x_1^2 + x_2^2 + \cdots + x_N^2}{N}
$$

So we can write the variance as
$$
\begin{bmatrix}
x_1 & x_2 & \cdots & x_N 
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_N 
\end{bmatrix} = 
X' X
$$

- Covariance matrices
- Ideal point and matrix decomposition
- Linear regression representation

## Standard Errors 

$\hat{}
